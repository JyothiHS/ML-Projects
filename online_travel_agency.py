# -*- coding: utf-8 -*-
"""Online Travel Agency.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L8Mo3kD7hdD8LlVcQXkLKYcQf8f9GSFm
"""

# Commented out IPython magic to ensure Python compatibility.
# import all libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as sp
# %matplotlib inline

df = pd.read_csv('/content/drive/MyDrive/train_small.csv',date_parser=[-1])   #load the dataset

df.head()   # get top 5 rows of dataframe

df.tail()   # get last 5 rows

df.shape    # number of rows and columns

df.info() # inforamtion about datatypes of each column

df.columns  # columns names

df.isnull().sum()  # check the total null values in dataset

"""**find null values**"""

percent_null= df.isna().sum().sort_values(ascending=False)/len(df)*100  #percentage of all null values
percent_null

df.isnull().sum().sum() # total number of null values

""" **handling null values**
 
 In this dataset we got more than 50% of the null values so we can drop those columns


"""

# drop some columns which contains large number of null values
df.drop(['log_click_proportion','user_hist_paid','user_hist_stars','booking_value',
 'competitor1_rate', 'competitor1_has_availability','search_id','listing_id',
       'competitor1_price_percent_diff', 'competitor2_rate','timestamp',
       'competitor2_has_availability', 'competitor2_price_percent_diff',
       'competitor3_rate', 'competitor3_has_availability',
       'competitor3_price_percent_diff', 'competitor4_rate',
       'competitor4_has_availability', 'competitor4_price_percent_diff',
       'competitor5_rate', 'competitor5_has_availability',
       'competitor5_price_percent_diff', 'competitor6_rate',
       'competitor6_has_availability', 'competitor6_price_percent_diff',
       'competitor7_rate', 'competitor7_has_availability',
       'competitor7_price_percent_diff', 'competitor8_rate',
       'competitor8_has_availability', 'competitor8_price_percent_diff'], inplace= True,axis=1)

df.shape  #get number of columns and rows

df.isnull().sum()

"""still we got some null values so we can replace some values for these columns
they are listing_review_score,distance_to_dest and  
"""

df['listing_review_score'].fillna(0,inplace=True)

"""replace null values of 'listing_review_score' with 0's

"""

mean_score = df['location_score2'].mean()
df['location_score2'].fillna(mean_score,inplace =True)

median_dist=df['distance_to_dest'].median()
 df['distance_to_dest'].fillna(median_dist,inplace=True)

"""replace null values of 'distance_to_dest' with median of that column."""

df.isnull().sum() #check null values present it or not

"""finally we have '0' null values in all columns

**describe()**

retuens description of the data in the dataframe. if the dataframe contains numerical data, the description contains statistical value of that colums.. like mean,std, min value, max value, percentile values and also count.
"""

df.describe().T

"""**correlation matrix**"""

df.corr() # check the correlation of each numerical columns

"""**heat map**"""

plt.figure(figsize=(16,10))
sns.heatmap(df.corr(),linewidths=3,annot = True)
plt.show()

pip install dataprep

"""dataset insights of all columns"""

from dataprep.eda import plot,plot_correlation, plot_missing
plot(df)

"""**outliers**"""

def outlier(var):
  q1= var.quantile(0.25)
  q2=var.quantile(0.50)
  q3=var.quantile(0.75)
  IQR =q3-q1
  lc=q1-1.5*IQR
  uc=q3+1.5*IQR
  print('IQR', IQR,"lower count",lc,"upper count",uc)
  out_plot = plt.boxplot(var)
  return lc,uc,out_plot

print("outlier of clicked=  ",outlier(df['clicked']))

q1= df['booked'].quantile(0.25)
q2=df['booked'].quantile(0.50)
q3=df['booked'].quantile(0.75)
IQR =q3-q1
lc=q1-1.5*IQR
uc=q3+1.5*IQR
remove_outlier=df[(df['booked']<lc) | (df['booked']>uc)]
plt.boxplot(remove_outlier['booked'])

q1= df['clicked'].quantile(0.25)
q2=df['clicked'].quantile(0.50)
q3=df['clicked'].quantile(0.75)
IQR =q3-q1
lc=q1-1.5*IQR
uc=q3+1.5*IQR
remove_outlier=df[(df['clicked']<lc) | (df['clicked']>uc)]
plt.boxplot(remove_outlier['clicked'])

q1= df['distance_to_dest'].quantile(0.25)
q2=df['distance_to_dest'].quantile(0.50)
q3=df['distance_to_dest'].quantile(0.75)
IQR =q3-q1
lc=q1-1.5*IQR
uc=q3+1.5*IQR
remove_outlier=df[(df['distance_to_dest']<lc) | (df['distance_to_dest']>uc)]
plt.boxplot(remove_outlier['distance_to_dest'])

q1= df['length_of_stay'].quantile(0.25)
q2=df['length_of_stay'].quantile(0.50)
q3=df['length_of_stay'].quantile(0.75)
IQR =q3-q1
lc=q1-1.5*IQR
uc=q3+1.5*IQR
remove_outlier=df[(df['length_of_stay']<lc) | (df['length_of_stay']>uc)]
plt.boxplot(remove_outlier['length_of_stay'])

"""**EDA**"""

df['booked'].value_counts() # values counts of 'booked' columns, by seeing this we can say that dataset is imbalanced

sns.countplot(x='booked',data=df, palette='hls')
plt.show();
df['booked'].value_counts()

df['clicked'].value_counts() # data is imbalanced

sns.countplot(x='clicked',data=df, palette='hls')
plt.show();
df['clicked'].value_counts()

"""**train and test split**"""

sns.countplot(x='num_rooms',hue='booked', data=df)

"""****

**length_of_stay**
"""

n, bins, patches = plt.hist(df.length_of_stay, 50, density = 1, facecolor='blue', alpha=0.75)
plt.xlabel('Search length of stay')
plt.title('Histogram of search_length_of_stay')
plt.axis([0, 30, 0, 0.65])
plt.show();

df.groupby('length_of_stay').size().nlargest(5)

"""The most searched length_of_stay is 1 day, then 2, 3...."""

n, bins, patches = plt.hist(df.num_adults, 20, density = 1, facecolor='blue', alpha=0.75)
plt.xlabel('Search adults count')
plt.title('Histogram of number of adults')
plt.show();

df.groupby('num_adults').size().nlargest(5)

"""The most common search adults count is 2-adults, then 1-adult"""

n, bins, patches = plt.hist(df.listing_stars, 20, density = 1, facecolor='blue', alpha=0.75)
plt.xlabel('star rating')
plt.title('Histogram of listing_star_rating')
plt.show();

"""The most common searched property star rating is 3 stars"""

df.groupby('is_brand').size()

"""More than 70% of the properties are brand properties

**stay on saturday**
"""

df.groupby('stay_on_saturday').size()

"""**price usd**"""

sns.set(style="ticks", palette="pastel")
ax = sns.boxplot(x="clicked", y="price_usd", hue="clicked", data=df)
ax.set_ylim([0, 200]);

df.head()

df1 =df

X = df1.drop(['booked', 'clicked'], axis = 1)
y =df1['clicked']

X.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2, train_size=.8)

X_train

X_test

y_train.value_counts()

y_train.shape

y_test.value_counts()

print(X.ndim)
print(y.ndim)

"""**transformation**"""

from sklearn.preprocessing import StandardScaler
scale=StandardScaler()
scale.fit_transform(X_train)
scale.transform(X_test)

!pip install imblearn

from imblearn.over_sampling import SMOTE

print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0)))
  
# import SMOTE module from imblearn library
# pip install imblearn (if you don't have imblearn in your system)
sm = SMOTE(random_state = 2)
X_train, y_train = sm.fit_resample(X_train, y_train.ravel())
  
print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train.shape))
  
print("After OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train== 0)))

X_train.shape

"""**logistic regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
LR

yhat = LR.predict(X_test)  #predict using test set
yhat

yhat_prob = LR.predict_proba(X_test)
yhat_prob

from sklearn.metrics import classification_report, confusion_matrix
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
print(confusion_matrix(y_test, yhat, labels=[1,0]))

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])
np.set_printoptions(precision=2)
# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=['clicked=1','clicked=0'],normalize= False,title='Confusion matrix')

print (classification_report(y_test, yhat))

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

def get_ensemble_models():
    rf =RandomForestClassifier(n_estimators=51,min_samples_leaf=5,min_samples_split=3)
    ada = AdaBoostClassifier(n_estimators=51,random_state=42)
    grad = GradientBoostingClassifier(n_estimators=101,random_state=42)
    classifier_list = [rf,ada,grad]
    classifier_name_list = ['Random Forests','AdaBoost','Gradient Boost']
    return classifier_list,classifier_name_list

def print_evaluation_metrics(trained_model,trained_model_name,X_test,y_test):
    print('--------- Model : ', trained_model_name, ' ---------------\n')
    predicted_values = trained_model.predict(X_test)
    print(metrics.classification_report(y_test,predicted_values))
    print("Accuracy Score : ",metrics.accuracy_score(y_test,predicted_values))
    print("---------------------------------------\n")

classifier_list, classifier_name_list = get_ensemble_models()
for classifier,classifier_name in zip(classifier_list,classifier_name_list):
    classifier.fit(X_train,y_train)
    print_evaluation_metrics(classifier,classifier_name,X_test,y_test)

"""**Click-through prediction with Naive bayes models**"""

def get_naive_bayes_models():
    gnb = GaussianNB()
    mnb = MultinomialNB()
    bnb = BernoulliNB()
    classifier_list = [gnb,mnb,bnb]
    classifier_name_list = ['Gaussian NB','Multinomial NB','Bernoulli NB']
    return classifier_list,classifier_name_list

classifier_list, classifier_name_list = get_naive_bayes_models()
for classifier,classifier_name in zip(classifier_list,classifier_name_list):
    classifier.fit(X_train,y_train)
    print_evaluation_metrics(classifier,classifier_name,X_test,y_test)

rf = RandomForestClassifier(n_estimators=51,min_samples_leaf=5,min_samples_split=3)
rf.fit(X_train, y_train)
(pd.Series(rf.feature_importances_, index=X.columns).plot(kind='barh'))